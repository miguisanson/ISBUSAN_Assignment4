# --- Reddit Scraper for Olympics Project Analysis ---
# Install required packages if you haven't already
# install.packages("RedditExtractoR")
# install.packages("dplyr")
# Load the necessary libraries
library(RedditExtractoR)
library(dplyr)
# --- Configuration ---
# 1. Set the output directory for the final CSV files.
# IMPORTANT: Make sure this folder exists on your computer before running the script.
setwd("E:/Github School Projects/ISBUSAN_Database_Assets/Reddit_CSV")
# 2. Define the keywords for the two separate exports.
sports_keywords <- c("Basketball", "Swimming", "Gymnastics", "Weightlifting", "Tennis")
general_keyword <- "Olympics"
n_posts <- 100 # The number of posts to scrape for EACH keyword.
subreddit_name <- "olympics"
# --- Helper Functions ---
# This function removes special characters that can cause errors.
clean_text <- function(text) {
# Replaces non-ASCII characters with an empty string
iconv(text, "latin1", "ASCII", sub = "")
}
# This function extracts the main post and all its comments from a single URL.
# It's defined once here and used in both scraping sections below.
extract_thread_data <- function(thread_url, search_keyword) {
tryCatch({
content <- get_thread_content(thread_url)
post_df <- content$threads
comments_df <- content$comments
if (is.null(post_df) || nrow(post_df) == 0) return(NULL)
# Format the original post data, now with text cleaning
original_post <- post_df %>%
transmute(
search_keyword = search_keyword,
post_url = url,
title = clean_text(title),
author = clean_text(author),
date = date,
text = clean_text(text),
score = score,
type = "post"
)
# Check for and format comments, now with text cleaning
if (!is.null(comments_df) && nrow(comments_df) > 0) {
comments <- comments_df %>%
transmute(
search_keyword = search_keyword,
post_url = thread_url,
title = clean_text(post_df$title[1]),
author = clean_text(author),
date = date,
text = clean_text(comment),
score = score,
type = "comment"
)
return(bind_rows(original_post, comments))
} else {
return(original_post)
}
}, error = function(e) {
message(paste("Skipped thread due to error:", thread_url, "-", e$message))
return(NULL)
})
}
# --- Part 1: Scrape the 5 Specific Sports ---
cat("--- Starting Part 1: Scraping 5 Specific Sports ---\n")
all_sports_data <- list()
for (keyword in sports_keywords) {
cat(paste0("\n--- Searching for threads with keyword: '", keyword, "' ---\n"))
threads <- NULL
retries <- 0
max_retries <- 5
retry_delay <- 15 # Start with a 15-second delay
# NEW: This loop will keep retrying if it gets rate-limited, instead of skipping.
while(is.null(threads) && retries < max_retries) {
threads <- tryCatch({
find_thread_urls(keywords = keyword, subreddit = subreddit_name, sort_by = "relevance", period = "all")
}, error = function(e) {
retries <<- retries + 1
message(paste("Could not fetch threads for '", keyword, "' due to an error: ", e$message))
message(paste("This is likely due to rate limiting. Retrying in", retry_delay, "seconds... (Attempt", retries, "of", max_retries, ")"))
Sys.sleep(retry_delay)
retry_delay <<- retry_delay * 2 # Double the delay for the next attempt
return(NULL)
})
}
# Check if the search was successful after retries
if (is.null(threads) || !is.data.frame(threads) || nrow(threads) == 0) {
cat(paste0("Failed to fetch threads for '", keyword, "' after ", max_retries, " attempts. Moving to next keyword.\n"))
next
}
threads <- head(threads, n_posts)
cat(paste0("Found ", nrow(threads), " threads for '", keyword, "'. Starting extraction...\n"))
for (i in seq_along(threads$url)) {
url <- threads$url[i]
cat(paste0("Processing post ", i, " of ", nrow(threads), ": ", url, "\n"))
thread_data <- extract_thread_data(url, keyword)
if (!is.null(thread_data)) {
all_sports_data[[length(all_sports_data) + 1]] <- thread_data
}
# Increased and randomized the delay to be more 'polite' to Reddit's servers
Sys.sleep(sample(3:5, 1))
}
}
# Combine and save the data for the 5 sports
if (length(all_sports_data) > 0) {
final_sports_df <- bind_rows(all_sports_data)
sports_output_file <- paste0("reddit_", subreddit_name, "_", "5-sports_data.csv")
write.csv(final_sports_df, sports_output_file, row.names = FALSE, fileEncoding = "UTF-8")
cat("\nPart 1 Complete! Sports data saved to:", file.path(getwd(), sports_output_file), "\n")
} else {
cat("\nPart 1 Complete, but no data was successfully scraped for the specific sports.\n")
}
# --- Part 2: Scrape General Olympics Discussion ---
cat("\n--- Starting Part 2: Scraping General 'Olympics' Discussion ---\n")
all_general_data <- list()
threads <- NULL
retries <- 0
retry_delay <- 15
while(is.null(threads) && retries < max_retries) {
threads <- tryCatch({
find_thread_urls(keywords = general_keyword, subreddit = subreddit_name, sort_by = "relevance", period = "all")
}, error = function(e) {
retries <<- retries + 1
message(paste("Could not fetch threads for '", general_keyword, "' due to an error: ", e$message))
message(paste("This is likely due to rate limiting. Retrying in", retry_delay, "seconds... (Attempt", retries, "of", max_retries, ")"))
Sys.sleep(retry_delay)
retry_delay <<- retry_delay * 2
return(NULL)
})
}
if (is.null(threads) || !is.data.frame(threads) || nrow(threads) == 0) {
cat(paste0("Failed to fetch threads for '", general_keyword, "' after ", max_retries, " attempts.\n"))
} else {
threads <- head(threads, n_posts)
cat(paste0("Found ", nrow(threads), " threads for '", general_keyword, "'. Starting extraction...\n"))
for (i in seq_along(threads$url)) {
url <- threads$url[i]
cat(paste0("Processing post ", i, " of ", nrow(threads), ": ", url, "\n"))
thread_data <- extract_thread_data(url, general_keyword)
if (!is.null(thread_data)) {
all_general_data[[length(all_general_data) + 1]] <- thread_data
}
Sys.sleep(sample(3:5, 1))
}
}
# Combine and save the data for the general discussion
if (length(all_general_data) > 0) {
final_general_df <- bind_rows(all_general_data)
general_output_file <- paste0("reddit_", subreddit_name, "_", "general-discussion_data.csv")
write.csv(final_general_df, general_output_file, row.names = FALSE, fileEncoding = "UTF-8")
cat("\nPart 2 Complete! General discussion data saved to:", file.path(getwd(), general_output_file), "\n")
} else {
cat("\nPart 2 Complete, but no data was successfully scraped for the general keyword.\n")
}
cat("\n--- All scraping finished! ---\n")
# --- Olympics Reddit Sentiment Analysis ---
# This script reads the scraped Reddit data, performs sentiment analysis,
# and generates summary visualizations and word clouds.
# --- Part 0: Setup ---
# Install the necessary packages if you haven't already.
# install.packages("dplyr")
# install.packages("tidytext")
# install.packages("tidyr")
# install.packages("ggplot2")
# install.packages("wordcloud")
# install.packages("RColorBrewer")
# Load the required libraries
library(dplyr)
library(tidytext)
library(tidyr)
library(ggplot2)
library(wordcloud)
library(RColorBrewer)
# --- Part 1: Load and Prepare Data ---
# Set your working directory to where your CSV files are located.
setwd("E:/Github School Projects/ISBUSAN_Database_Assets/Reddit_CSV")
# Load the two datasets you created with the scraper script
sports_data <- read.csv("reddit_olympics_5-sports_data.csv")
general_data <- read.csv("reddit_olympics_general-discussion_data.csv")
# Combine them into a single dataframe for analysis
full_data <- bind_rows(sports_data, general_data)
# Prepare the data for analysis
# 1. Add a unique ID to every row (post or comment)
# 2. Combine the post title and the text for better context
prepared_data <- full_data %>%
mutate(doc_id = row_number()) %>%
mutate(full_text = paste(title, text)) %>%
select(doc_id, search_keyword, full_text, score)
cat("Data loaded and prepared successfully.\n")
# --- Part 2: Perform Sentiment Analysis ---
# We will use the 'bing' sentiment lexicon, which categorizes words
# as either "positive" or "negative".
# Get the Bing lexicon
bing_lexicon <- get_sentiments("bing")
# Perform the sentiment analysis
sentiment_scores <- prepared_data %>%
# Break the text into individual words (tokens)
unnest_tokens(word, full_text) %>%
# Join with the sentiment lexicon to tag each word
inner_join(bing_lexicon, by = "word") %>%
# Count the number of positive and negative words for each document
count(doc_id, sentiment) %>%
# Create separate columns for "positive" and "negative" counts
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
# Calculate a final sentiment score (positive - negative)
mutate(sentiment_score = positive - negative)
# Join the sentiment scores back to our main prepared data
final_data <- prepared_data %>%
left_join(sentiment_scores, by = "doc_id") %>%
# If a document had no sentiment words, its score will be NA. Change this to 0.
mutate(
positive = ifelse(is.na(positive), 0, positive),
negative = ifelse(is.na(negative), 0, negative),
sentiment_score = ifelse(is.na(sentiment_score), 0, sentiment_score)
)
cat("Sentiment analysis complete.\n")
# --- Part 3: Summarize and Visualize Results ---
# Create a summary of sentiment distribution for each sport
sentiment_summary <- final_data %>%
# Classify each document's overall sentiment
mutate(overall_sentiment = case_when(
sentiment_score > 0 ~ "Positive",
sentiment_score < 0 ~ "Negative",
TRUE ~ "Neutral"
)) %>%
# Count the number of Positive/Negative/Neutral posts for each keyword
count(search_keyword, overall_sentiment) %>%
# Calculate percentages for better comparison
group_by(search_keyword) %>%
mutate(percentage = n / sum(n) * 100)
# Print the summary table to the console
print(sentiment_summary)
# Create a bar chart to visualize the sentiment comparison
sentiment_plot <- ggplot(sentiment_summary, aes(x = search_keyword, y = percentage, fill = overall_sentiment)) +
geom_bar(stat = "identity", position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(
title = "Sentiment Distribution Across Olympic Topics on Reddit",
x = "Search Keyword",
y = "Percentage of Posts/Comments",
fill = "Overall Sentiment"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Display the plot. It will also be saved as a file.
print(sentiment_plot)
# Save the plot as a PNG file
ggsave("sentiment_distribution_plot.png", plot = sentiment_plot, width = 10, height = 6)
cat("Sentiment distribution plot saved as sentiment_distribution_plot.png\n")
# Save the final, detailed data with sentiment scores to a new CSV.
# This file is ready to be used in Tableau.
write.csv(final_data, "reddit_data_with_sentiment.csv", row.names = FALSE)
cat("Final data with sentiment scores saved to reddit_data_with_sentiment.csv\n")
# --- Part 4: Generate Word Clouds ---
# This section will create separate positive and negative word clouds for each sport.
# Get the list of unique keywords to loop through
keywords <- unique(full_data$search_keyword)
# Tokenize the full dataset once for efficiency
tokenized_data <- prepared_data %>%
unnest_tokens(word, full_text) %>%
# Remove common "stopwords" like 'the', 'a', 'is', etc.
anti_join(stop_words, by = "word")
for (sport in keywords) {
cat(paste("Generating word clouds for:", sport, "\n"))
# Filter for the current sport and join with the sentiment lexicon
sport_words <- tokenized_data %>%
filter(search_keyword == sport) %>%
inner_join(bing_lexicon, by = "word")
# Create Positive and Negative clouds
for (senti in c("positive", "negative")) {
# Prepare the word frequency data
word_freq <- sport_words %>%
filter(sentiment == senti) %>%
count(word, sort = TRUE)
# Check if there are any words for this sentiment
if (nrow(word_freq) > 0) {
# Set the filename for the plot
png(paste0(sport, "_", senti, "_word_cloud.png"), width = 800, height = 800)
# Generate the word cloud
wordcloud(
words = word_freq$word,
freq = word_freq$n,
min.freq = 2,
max.words = 100,
random.order = FALSE,
rot.per = 0.35,
colors = brewer.pal(8, "Dark2")
)
# Add a title
title(main = paste(sport, "-", toupper(senti), "WORDS"))
dev.off() # Close the PNG device
}
}
}
cat("Word cloud generation complete.\n")
